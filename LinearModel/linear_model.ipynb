{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "$$\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "$$ L = \\frac{1}{2}\\sum_{i=1}^n(y_i - \\hat{y_i})^2\\tag{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据,数据有100条记录，每条记录有两个特征$x_1$和$x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([[1.4], [2.3]], dtype = np.float64)\n",
    "true_b = 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, (100,2))\n",
    "zao = np.random.randn(100,1)\n",
    "y = np.dot(X, true_w) + true_b + zao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了数据，特征$X$和对应的关注值$y$，我们要如何去得到线性模型的参数$w$和$b$呢?  \n",
    "若记$\\bar{w}=\\left[\\begin{matrix}w \\\\ b \\end{matrix}\\right]$,$\\bar{X}=\\left[\\begin{matrix}X & 1 \\end{matrix}\\right]$对于统计学，你可以推导出来最优解 $\\hat{\\bar{w}}^*=(\\bar{X}^\\intercal\\bar{X})^{-1}(\\bar{X}^\\intercal)y$。但是这个方法有很大的局限性  \n",
    "1. X必须是可逆的\n",
    "2. 当数据量和特征维度都很大是，计算耗时，求逆更耗时  \n",
    "\n",
    "那么有别的办法吗？有！从优化的角度来做。这是个凸函数，用梯度下降的方法，一定可以找到全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先计算损失函数对参数的偏导数  \n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)x_{i1}\\tag{3}$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，若定义$\\vec{w}=\\left[\\begin{matrix}w1 \\\\ w2 \\\\ b \\end{matrix}\\right]$,那么  \n",
    "$$\\frac{\\partial{L}}{\\partial{\\vec{w}}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\left[\\begin{matrix}x_{i1}\\\\x_{i2}\\\\1 \\end{matrix}\\right]\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写成矩阵形式，对$w$的计算为, $X_{100\\times2}$ ,$y_{100\\times1}$,$\\hat{y}_{100\\times1}$\n",
    "$$\\frac{\\partial{L}}{\\partial{\\left[\\begin{matrix}w_1\\\\w_2 \\end{matrix}\\right]}} = X^\\intercal(\\hat{y}-y)\\tag{5}$$  \n",
    "对$y$的计算为,其中$\\vec{1}$元素全为1的列向量\n",
    "$$\\frac{\\partial{L}}{\\partial{y}} = {\\vec{1}}^\\intercal(\\hat{y}-y)\\tag{5}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先随机初始化$w$和$b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.963915  ],\n",
       "        [-0.63304225]]), array([-0.040506]))"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.randn(2,1)\n",
    "b = np.random.randn(1)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据推导的公式计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-431.78921776],\n",
       "        [-338.20905788]]), array([[-526.9522258]]))"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_grad(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而深度学习框架都提供了自动求导机制，省去了手动推导公式的繁琐，对于神经网络等更有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd, autograd\n",
    "ndX = nd.array(X)\n",
    "ndy = nd.array(y)\n",
    "ndzao = nd.array(zao)\n",
    "ndtrue_w = nd.array(true_w)\n",
    "ndtrue_b = nd.array([true_b])\n",
    "ndw = nd.array(w)\n",
    "ndb = nd.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mxnet的变量自动求导，需要先给其分配内存记录\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    los = (ndy-(nd.dot(ndX, ndw) +  ndb))**2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "los.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[-431.7892]\n",
       "  [-338.209 ]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [-526.9522]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw.grad,ndb.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，使用mxnet的自动求导机制算得的梯度，和我们手动计算的梯度是一致的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用梯度下降训练模型，这里先定义几个函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w, b):\n",
    "    return 1/2*(np.dot(X,w)+b-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求梯度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: 10.748258895560653\n",
      "10 th iter, loss is: 0.5551090349265329\n",
      "20 th iter, loss is: 0.552559886859715\n",
      "30 th iter, loss is: 0.5525582376749849\n",
      "40 th iter, loss is: 0.5525582354808939\n",
      "50 th iter, loss is: 0.5525582354771611\n",
      "60 th iter, loss is: 0.5525582354771544\n",
      "70 th iter, loss is: 0.5525582354771545\n",
      "80 th iter, loss is: 0.5525582354771545\n",
      "90 th iter, loss is: 0.5525582354771544\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = X.shape[0]\n",
    "for i in range(num_epochs):\n",
    "    w_grad, b_grad = get_grad(X,y,w,b)\n",
    "    w = w - lr*w_grad/N\n",
    "    b = b - lr*b_grad/N\n",
    "    los = loss(X, y, w, b)\n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', np.mean(los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.27517596],\n",
       "        [2.26746547]]), array([[4.72462129]]))"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.4],\n",
       "        [2.3]]), 4.7)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w,true_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面实现随机梯度下降法来计算模型参数  \n",
    "首先分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter1(batch_size, X, y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = np.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X[j], y[j]  # take 函数根据索引返回对应元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.10425416  0.53744733]\n",
      " [ 1.41665835 -0.8384792 ]\n",
      " [ 1.23419133  0.7761752 ]\n",
      " [-0.58011035  1.00976837]\n",
      " [ 0.92207333 -0.17040899]\n",
      " [-0.76202002 -0.82587629]\n",
      " [ 0.47843025  0.86187774]\n",
      " [-0.10212744 -0.73475063]\n",
      " [ 0.0365292   2.09224611]\n",
      " [ 1.31619544  0.77441899]] [[4.09030254]\n",
      " [3.80566454]\n",
      " [8.99708103]\n",
      " [8.63543191]\n",
      " [5.53068968]\n",
      " [1.09725473]\n",
      " [9.02026778]\n",
      " [3.08251743]\n",
      " [8.37406336]\n",
      " [6.60325469]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for i,j in data_iter(batch_size, X, y):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新初始化$w$,$b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(2,1)\n",
    "b = np.random.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: 0.5564595708043266\n",
      "10 th iter, loss is: 0.5698418517348373\n",
      "20 th iter, loss is: 0.5982968558560376\n",
      "30 th iter, loss is: 0.5557177891091425\n",
      "40 th iter, loss is: 0.5626929669253424\n",
      "50 th iter, loss is: 0.5679093231445218\n",
      "60 th iter, loss is: 0.583278473915394\n",
      "70 th iter, loss is: 0.672769470699366\n",
      "80 th iter, loss is: 0.5660121623422896\n",
      "90 th iter, loss is: 0.5744911931686882\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = X.shape[0]\n",
    "for i in range(num_epochs):\n",
    "    for X_s,y_s in data_iter(batch_size, X, y):\n",
    "        w_grad, b_grad = get_grad(X_s,y_s,w,b)\n",
    "        w = w - lr*w_grad/batch_size\n",
    "        b = b - lr*b_grad/batch_size\n",
    "    los = loss(X, y, w, b)\n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', np.mean(los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.33977475],\n",
       "        [2.16482354]]), array([[4.74738742]]))"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Mxnet来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, w, b):\n",
    "    return nd.dot(X,w)+b\n",
    "def myloss(y,yhat):\n",
    "    return (y-yhat)**2/2\n",
    "def sgd(params, lr, batch_size):  # 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.randn(2,1)\n",
    "ndb = nd.random.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[-1.1468065 ]\n",
       "  [ 0.05383794]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[-2.5074806]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = ndX.shape[0]\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有一个注意点，如果更新参数$ndw$时使用$ndw$而不是使用$ndw[:]$，需要重新申请存储梯度的内存，这是因为ndw已经指向了新的内存空间，这个变量之前申请的存储梯度内存已经失效。而$ndw[:]$的值则是写入了原来变量的内存空间，所以不需要再次申请存储梯度的内存空间（多说一句，这和python的list有所不同，$a = [1,2,3,4]$，$a[:]$是指向了新的内存空间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: [15.969685]\n",
      "10 th iter, loss is: [0.5610953]\n",
      "20 th iter, loss is: [0.5525695]\n",
      "30 th iter, loss is: [0.55255824]\n",
      "40 th iter, loss is: [0.55255824]\n",
      "50 th iter, loss is: [0.55255824]\n",
      "60 th iter, loss is: [0.55255824]\n",
      "70 th iter, loss is: [0.55255824]\n",
      "80 th iter, loss is: [0.55255824]\n",
      "90 th iter, loss is: [0.55255824]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    with autograd.record():\n",
    "        los = (ndy-(nd.dot(ndX, ndw) +  ndb))**2/2\n",
    "    los.backward()\n",
    "    ndw[:] = ndw-lr*ndw.grad/N\n",
    "    ndb[:] = ndb-lr*ndb.grad/N\n",
    "    '''ndw = ndw-lr*ndw.grad/N\n",
    "    ndb = ndb-lr*ndb.grad/N\n",
    "    ndw.attach_grad()\n",
    "    ndb.attach_grad()'''\n",
    "    \n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', ((ndy-(nd.dot(ndX, ndw) +  ndb))**2/2).mean().asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.275176 ]\n",
       "  [2.2674658]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.724621]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mxnet的随机梯度下降实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter2(batch_size, X, y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X.take(j), y.take(j)  # take 函数根据索引返回对应元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.623145   -0.75702196]\n",
      " [ 0.47843024  0.86187774]\n",
      " [ 2.6484764  -0.79903346]\n",
      " [ 0.42794493  0.05743427]\n",
      " [ 0.2446903  -1.1968386 ]\n",
      " [ 0.4478474  -0.7138504 ]\n",
      " [-0.03712387 -1.321181  ]\n",
      " [-1.6545358   0.6468618 ]\n",
      " [ 0.37854886  0.32565856]\n",
      " [ 1.0117495  -0.14863871]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[[2.351377 ]\n",
      " [9.0202675]\n",
      " [6.506955 ]\n",
      " [5.695367 ]\n",
      " [4.7705984]\n",
      " [4.013717 ]\n",
      " [1.9680197]\n",
      " [5.858164 ]\n",
      " [7.810449 ]\n",
      " [3.9057903]]\n",
      "<NDArray 10x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for i,j in data_iter2(batch_size, ndX, ndy):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新初始化ndw和ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.randn(2,1)\n",
    "ndb = nd.random.randn(1,1)\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 8.968078\n",
      "epoch 2, loss 4.796886\n",
      "epoch 3, loss 2.716576\n",
      "epoch 4, loss 1.654661\n",
      "epoch 5, loss 1.127365\n",
      "epoch 6, loss 0.852734\n",
      "epoch 7, loss 0.706986\n",
      "epoch 8, loss 0.633091\n",
      "epoch 9, loss 0.596218\n",
      "epoch 10, loss 0.575363\n",
      "epoch 11, loss 0.564838\n",
      "epoch 12, loss 0.558930\n",
      "epoch 13, loss 0.556001\n",
      "epoch 14, loss 0.554506\n",
      "epoch 15, loss 0.553538\n",
      "epoch 16, loss 0.553136\n",
      "epoch 17, loss 0.552920\n",
      "epoch 18, loss 0.552715\n",
      "epoch 19, loss 0.552690\n",
      "epoch 20, loss 0.552614\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter(batch_size, ndX, ndy):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, ndw, ndb), y)  # l 是有关小批量 X 和 y 的损失。\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度。\n",
    "        sgd([ndw, ndb], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = loss(net(ndX, ndw, ndb), ndy)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.2825043]\n",
       "  [2.273849 ]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.7210484]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw,ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.normal(0,0.001,(2,1))\n",
    "ndb = nd.zeros((1,1))\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 8.700970\n",
      "epoch 2, loss 4.672668\n",
      "epoch 3, loss 2.640820\n",
      "epoch 4, loss 1.609280\n",
      "epoch 5, loss 1.094201\n",
      "epoch 6, loss 0.830468\n",
      "epoch 7, loss 0.696503\n",
      "epoch 8, loss 0.627280\n",
      "epoch 9, loss 0.590571\n",
      "epoch 10, loss 0.572705\n",
      "epoch 11, loss 0.563168\n",
      "epoch 12, loss 0.557713\n",
      "epoch 13, loss 0.555421\n",
      "epoch 14, loss 0.554061\n",
      "epoch 15, loss 0.553597\n",
      "epoch 16, loss 0.553018\n",
      "epoch 17, loss 0.552757\n",
      "epoch 18, loss 0.552693\n",
      "epoch 19, loss 0.552637\n",
      "epoch 20, loss 0.552626\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter(batch_size, ndX, ndy):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, ndw, ndb), y)  # l 是有关小批量 X 和 y 的损失。\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度。\n",
    "        sgd([ndw, ndb], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = loss(net(ndX, ndw, ndb), ndy)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.2804134]\n",
       "  [2.2662458]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.7144423]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

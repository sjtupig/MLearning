{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "$$\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "$$ L = \\frac{1}{2}\\sum_{i=1}^n(y_i - \\hat{y_i})^2\\tag{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据,数据有100条记录，每条记录有两个特征$x_1$和$x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([[1.4], [2.3]], dtype = np.float64)\n",
    "true_b = 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, (100,2))\n",
    "zao = np.random.normal(0, 0.001,(100,1))\n",
    "y = np.dot(X, true_w) + true_b + zao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了数据，特征$X$和对应的关注值$y$，我们要如何去得到线性模型的参数$w$和$b$呢?  \n",
    "若记$\\bar{w}=\\left[\\begin{matrix}w \\\\ b \\end{matrix}\\right]$,$\\bar{X}=\\left[\\begin{matrix}X & 1 \\end{matrix}\\right]$对于统计学，你可以推导出来最优解 $\\hat{\\bar{w}}^*=(\\bar{X}^\\intercal\\bar{X})^{-1}(\\bar{X}^\\intercal)y$。但是这个方法有很大的局限性  \n",
    "1. X必须是可逆的\n",
    "2. 当数据量和特征维度都很大是，计算耗时，求逆更耗时  \n",
    "\n",
    "那么有别的办法吗？有！从优化的角度来做。这是个凸函数，用梯度下降的方法，一定可以找到全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先计算损失函数对参数的偏导数  \n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)x_{i1}\\tag{3}$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，若定义$\\vec{w}=\\left[\\begin{matrix}w1 \\\\ w2 \\\\ b \\end{matrix}\\right]$,那么  \n",
    "$$\\frac{\\partial{L}}{\\partial{\\vec{w}}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\left[\\begin{matrix}x_{i1}\\\\x_{i2}\\\\1 \\end{matrix}\\right]\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写成矩阵形式，对$w$的计算为, $X_{100\\times2}$ ,$y_{100\\times1}$,$\\hat{y}_{100\\times1}$\n",
    "$$\\frac{\\partial{L}}{\\partial{\\left[\\begin{matrix}w_1\\\\w_2 \\end{matrix}\\right]}} = X^\\intercal(\\hat{y}-y)\\tag{5}$$  \n",
    "对$y$的计算为,其中$\\vec{1}$元素全为1的列向量\n",
    "$$\\frac{\\partial{L}}{\\partial{y}} = {\\vec{1}}^\\intercal(\\hat{y}-y)\\tag{5}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先随机初始化$w$和$b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.69316828],\n",
       "        [1.49954833]]), array([0.78372433]))"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.randn(2,1)\n",
    "b = np.random.randn(1)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据推导的公式计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-85.79259115],\n",
       "        [-48.88827012]]), array([[-387.33870906]]))"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_grad(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而深度学习框架都提供了自动求导机制，省去了手动推导公式的繁琐，对于神经网络等更有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd, autograd\n",
    "ndX = nd.array(X)\n",
    "ndy = nd.array(y)\n",
    "ndzao = nd.array(zao)\n",
    "ndtrue_w = nd.array(true_w)\n",
    "ndtrue_b = nd.array([true_b])\n",
    "ndw = nd.array(w)\n",
    "ndb = nd.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mxnet的变量自动求导，需要先给其分配内存记录\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    los = (ndy-(nd.dot(ndX, ndw) +  ndb))**2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "los.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[-85.792595]\n",
       "  [-48.888275]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [-387.3387]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw.grad,ndb.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，使用mxnet的自动求导机制算得的梯度，和我们手动计算的梯度是一致的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用梯度下降训练模型，这里先定义几个函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w, b):\n",
    "    return 1/2*(np.dot(X,w)+b-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求梯度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: 4.005747715722106\n",
      "10 th iter, loss is: 0.004760808917494303\n",
      "20 th iter, loss is: 1.1206034496498005e-05\n",
      "30 th iter, loss is: 4.4904229915226564e-07\n",
      "40 th iter, loss is: 4.0541935284359094e-07\n",
      "50 th iter, loss is: 4.051967880069255e-07\n",
      "60 th iter, loss is: 4.0519558993317587e-07\n",
      "70 th iter, loss is: 4.051955834153209e-07\n",
      "80 th iter, loss is: 4.051955833797791e-07\n",
      "90 th iter, loss is: 4.051955833795651e-07\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = X.shape[0]\n",
    "for i in range(num_epochs):\n",
    "    w_grad, b_grad = get_grad(X,y,w,b)\n",
    "    w = w - lr*w_grad/N\n",
    "    b = b - lr*b_grad/N\n",
    "    los = loss(X, y, w, b)\n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', np.mean(los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.39998999],\n",
       "        [2.29991362]]), array([[4.70003541]]))"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.4],\n",
       "        [2.3]]), 4.7)"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w,true_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面实现随机梯度下降法来计算模型参数  \n",
    "首先分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter1(batch_size, X, y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = np.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X[j], y[j]  # take 函数根据索引返回对应元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3233138  -1.4846215 ]\n",
      " [ 0.44555086 -1.35650448]\n",
      " [ 0.82450613  0.14168134]\n",
      " [ 0.32114495 -0.18607105]\n",
      " [ 0.78150391 -0.26899311]\n",
      " [-2.14545196  0.44561945]\n",
      " [ 0.32489975 -0.43525001]\n",
      " [-0.13669077 -0.03272827]\n",
      " [ 0.61720557 -0.42424093]\n",
      " [ 1.22565425 -0.49575901]] [[0.8316827 ]\n",
      " [2.203336  ]\n",
      " [6.17885369]\n",
      " [4.72081294]\n",
      " [5.1757113 ]\n",
      " [2.72212542]\n",
      " [4.15458222]\n",
      " [4.4321947 ]\n",
      " [4.58706081]\n",
      " [5.27548265]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for i,j in data_iter(batch_size, X, y):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新初始化$w$,$b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(2,1)\n",
    "b = np.random.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: 0.012799319293725298\n",
      "10 th iter, loss is: 4.431399113513943e-07\n",
      "20 th iter, loss is: 4.06262070265052e-07\n",
      "30 th iter, loss is: 4.1230349422638e-07\n",
      "40 th iter, loss is: 4.123345896880925e-07\n",
      "50 th iter, loss is: 4.304242828911277e-07\n",
      "60 th iter, loss is: 4.141127151582546e-07\n",
      "70 th iter, loss is: 4.0703805816307525e-07\n",
      "80 th iter, loss is: 4.060119543184918e-07\n",
      "90 th iter, loss is: 4.0733200330309745e-07\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = X.shape[0]\n",
    "for i in range(num_epochs):\n",
    "    for X_s,y_s in data_iter(batch_size, X, y):\n",
    "        w_grad, b_grad = get_grad(X_s,y_s,w,b)\n",
    "        w = w - lr*w_grad/batch_size\n",
    "        b = b - lr*b_grad/batch_size\n",
    "    los = loss(X, y, w, b)\n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', np.mean(los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.39988949],\n",
       "        [2.29995442]]), array([[4.70001375]]))"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Mxnet来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, w, b):\n",
    "    return nd.dot(X,w)+b\n",
    "def myloss(y,yhat):\n",
    "    return (y-yhat)**2/2\n",
    "def sgd(params, lr, batch_size):  # 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.randn(2,1)\n",
    "ndb = nd.random.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[-1.5918757]\n",
       "  [-1.1081947]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[0.0787202]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = ndX.shape[0]\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有一个注意点，如果更新参数$ndw$时使用$ndw$而不是使用$ndw[:]$，需要重新申请存储梯度的内存，这是因为ndw已经指向了新的内存空间，这个变量之前申请的存储梯度内存已经失效。而$ndw[:]$的值则是写入了原来变量的内存空间，所以不需要再次申请存储梯度的内存空间（多说一句，这和python的list有所不同，$a = [1,2,3,4]$，$a[:]$是指向了新的内存空间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: [10.221592]\n",
      "10 th iter, loss is: [0.01048965]\n",
      "20 th iter, loss is: [1.1256972e-05]\n",
      "30 th iter, loss is: [4.167761e-07]\n",
      "40 th iter, loss is: [4.0519186e-07]\n",
      "50 th iter, loss is: [4.0520598e-07]\n",
      "60 th iter, loss is: [4.0520598e-07]\n",
      "70 th iter, loss is: [4.0520598e-07]\n",
      "80 th iter, loss is: [4.0520598e-07]\n",
      "90 th iter, loss is: [4.0520598e-07]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    with autograd.record():\n",
    "        los = (ndy-(nd.dot(ndX, ndw) +  ndb))**2/2\n",
    "    los.backward()\n",
    "    ndw[:] = ndw-lr*ndw.grad/N\n",
    "    ndb[:] = ndb-lr*ndb.grad/N\n",
    "    '''ndw = ndw-lr*ndw.grad/N\n",
    "    ndb = ndb-lr*ndb.grad/N\n",
    "    ndw.attach_grad()\n",
    "    ndb.attach_grad()'''\n",
    "    \n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', ((ndy-(nd.dot(ndX, ndw) +  ndb))**2/2).mean().asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.3999902]\n",
       "  [2.2999134]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.7000346]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet的随机梯度下降实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter2(batch_size, X, y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X.take(j), y.take(j)  # take 函数根据索引返回对应元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.4681375  -0.15534014]\n",
      " [ 0.26530302  0.3404787 ]\n",
      " [ 0.42121166 -1.0664554 ]\n",
      " [-1.551066   -0.38270727]\n",
      " [ 1.3770654   0.13794978]\n",
      " [ 0.5307424   0.35908428]\n",
      " [ 0.8445848  -0.54597986]\n",
      " [ 0.11354028 -0.4180504 ]\n",
      " [ 0.07860706 -1.2999861 ]\n",
      " [ 1.1178277  -0.14915887]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[[4.9992514]\n",
      " [5.8550897]\n",
      " [2.8376958]\n",
      " [1.6484268]\n",
      " [6.9457483]\n",
      " [6.2704363]\n",
      " [4.6253896]\n",
      " [3.8988945]\n",
      " [1.8206441]\n",
      " [5.9241266]]\n",
      "<NDArray 10x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for i,j in data_iter2(batch_size, ndX, ndy):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新初始化ndw和ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.randn(2,1)\n",
    "ndb = nd.random.randn(1,1)\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 13.256109\n",
      "epoch 2, loss 7.338350\n",
      "epoch 3, loss 4.063630\n",
      "epoch 4, loss 2.248680\n",
      "epoch 5, loss 1.245518\n",
      "epoch 6, loss 0.690749\n",
      "epoch 7, loss 0.383504\n",
      "epoch 8, loss 0.212836\n",
      "epoch 9, loss 0.118251\n",
      "epoch 10, loss 0.065795\n",
      "epoch 11, loss 0.036639\n",
      "epoch 12, loss 0.020418\n",
      "epoch 13, loss 0.011393\n",
      "epoch 14, loss 0.006371\n",
      "epoch 15, loss 0.003570\n",
      "epoch 16, loss 0.002003\n",
      "epoch 17, loss 0.001125\n",
      "epoch 18, loss 0.000634\n",
      "epoch 19, loss 0.000358\n",
      "epoch 20, loss 0.000202\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter(batch_size, ndX, ndy):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, ndw, ndb), y)  # l 是有关小批量 X 和 y 的损失。\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度。\n",
    "        sgd([ndw, ndb], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = loss(net(ndX, ndw, ndb), ndy)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.4003743]\n",
       "  [2.2887452]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.682356]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw,ndb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换一种参数初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.normal(0,0.001,(2,1))\n",
    "ndb = nd.zeros((1,1))\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 7.829946\n",
      "epoch 2, loss 4.346745\n",
      "epoch 3, loss 2.414821\n",
      "epoch 4, loss 1.343420\n",
      "epoch 5, loss 0.746863\n",
      "epoch 6, loss 0.415598\n",
      "epoch 7, loss 0.232045\n",
      "epoch 8, loss 0.129735\n",
      "epoch 9, loss 0.072646\n",
      "epoch 10, loss 0.040719\n",
      "epoch 11, loss 0.022895\n",
      "epoch 12, loss 0.012895\n",
      "epoch 13, loss 0.007277\n",
      "epoch 14, loss 0.004117\n",
      "epoch 15, loss 0.002338\n",
      "epoch 16, loss 0.001330\n",
      "epoch 17, loss 0.000760\n",
      "epoch 18, loss 0.000435\n",
      "epoch 19, loss 0.000250\n",
      "epoch 20, loss 0.000144\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter(batch_size, ndX, ndy):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, ndw, ndb), y)  # l 是有关小批量 X 和 y 的损失。\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度。\n",
    "        sgd([ndw, ndb], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = loss(net(ndX, ndw, ndb), ndy)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.4035972]\n",
       "  [2.2903054]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.685209]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Mxnet的高阶API Gluon来实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆一下模型的三要素\n",
    "1. 模型\n",
    "2. 损失函数\n",
    "3. 优化算法  \n",
    "  \n",
    "这几者在Gluon中都有对应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取数据，Gluon有data模块来将数据分批"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "??gdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? gdata.ArrayDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? gdata.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = gdata.ArrayDataset(ndX, ndy)\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init\n",
    "?? init.Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 7.844169\n",
      "epoch 2, loss: 4.352446\n",
      "epoch 3, loss: 2.417657\n",
      "epoch 4, loss: 1.342142\n",
      "epoch 5, loss: 0.747396\n",
      "epoch 6, loss: 0.416652\n",
      "epoch 7, loss: 0.232186\n",
      "epoch 8, loss: 0.129725\n",
      "epoch 9, loss: 0.072532\n",
      "epoch 10, loss: 0.040671\n",
      "epoch 11, loss: 0.022841\n",
      "epoch 12, loss: 0.012856\n",
      "epoch 13, loss: 0.007256\n",
      "epoch 14, loss: 0.004097\n",
      "epoch 15, loss: 0.002324\n",
      "epoch 16, loss: 0.001322\n",
      "epoch 17, loss: 0.000754\n",
      "epoch 18, loss: 0.000431\n",
      "epoch 19, loss: 0.000248\n",
      "epoch 20, loss: 0.000143\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "        l = loss(net(ndX), ndy)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.4]\n",
       "  [2.3]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [4.7]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndtrue_w, ndtrue_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = net[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1.403397  2.2903547]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[4.6852536]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

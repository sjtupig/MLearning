{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "$$\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "$$ L = \\frac{1}{2}\\sum_{i=1}^n(y_i - \\hat{y_i})^2\\tag{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据,数据有100条记录，每条记录有两个特征$x_1$和$x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([[1.4], [2.3]], dtype = np.float64)\n",
    "true_b = 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, (100,2))\n",
    "zao = np.random.normal(0, 0.001,(100,1))\n",
    "y = np.dot(X, true_w) + true_b + zao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了数据，特征$X$和对应的关注值$y$，我们要如何去得到线性模型的参数$w$和$b$呢?  \n",
    "若记$\\bar{w}=\\left[\\begin{matrix}w \\\\ b \\end{matrix}\\right]$,$\\bar{X}=\\left[\\begin{matrix}X & 1 \\end{matrix}\\right]$对于统计学，你可以推导出来最优解 $\\hat{\\bar{w}}^*=(\\bar{X}^\\intercal\\bar{X})^{-1}(\\bar{X}^\\intercal)y$。但是这个方法有很大的局限性  \n",
    "1. X必须是可逆的\n",
    "2. 当数据量和特征维度都很大是，计算耗时，求逆更耗时  \n",
    "\n",
    "那么有别的办法吗？有！从优化的角度来做。这是个凸函数，用梯度下降的方法，一定可以找到全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先计算损失函数对参数的偏导数  \n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)x_{i1}\\tag{3}$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，若定义$\\vec{w}=\\left[\\begin{matrix}w1 \\\\ w2 \\\\ b \\end{matrix}\\right]$,那么  \n",
    "$$\\frac{\\partial{L}}{\\partial{\\vec{w}}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\left[\\begin{matrix}x_{i1}\\\\x_{i2}\\\\1 \\end{matrix}\\right]\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写成矩阵形式，对$w$的计算为, $X_{100\\times2}$ ,$y_{100\\times1}$,$\\hat{y}_{100\\times1}$\n",
    "$$\\frac{\\partial{L}}{\\partial{\\left[\\begin{matrix}w_1\\\\w_2 \\end{matrix}\\right]}} = X^\\intercal(\\hat{y}-y)\\tag{5}$$  \n",
    "对$y$的计算为,其中$\\vec{1}$元素全为1的列向量\n",
    "$$\\frac{\\partial{L}}{\\partial{y}} = {\\vec{1}}^\\intercal(\\hat{y}-y)\\tag{5}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先随机初始化$w$和$b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.15392858],\n",
       "        [0.22306389]]), array([1.459993]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.randn(2,1)\n",
    "b = np.random.randn(1)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据推导的公式计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-105.49510806],\n",
       "        [-175.3691483 ]]), array([[-315.65400173]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_grad(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而深度学习框架都提供了自动求导机制，省去了手动推导公式的繁琐，对于神经网络等更有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd, autograd\n",
    "ndX = nd.array(X)\n",
    "ndy = nd.array(y)\n",
    "ndzao = nd.array(zao)\n",
    "ndtrue_w = nd.array(true_w)\n",
    "ndtrue_b = nd.array([true_b])\n",
    "ndw = nd.array(w)\n",
    "ndb = nd.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mxnet的变量自动求导，需要先给其分配内存记录\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    los = (ndy-(nd.dot(ndX, ndw) +  ndb))**2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "los.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[-105.4951 ]\n",
       "  [-175.36911]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [-315.654]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw.grad,ndb.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，使用mxnet的自动求导机制算得的梯度，和我们手动计算的梯度是一致的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用梯度下降训练模型，这里先定义几个函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w, b):\n",
    "    return 1/2*(np.dot(X,w)+b-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求梯度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: 3.9427338594420513\n",
      "10 th iter, loss is: 0.006435777080695627\n",
      "20 th iter, loss is: 1.3393769114546138e-05\n",
      "30 th iter, loss is: 5.644060828737941e-07\n",
      "40 th iter, loss is: 5.321654575407989e-07\n",
      "50 th iter, loss is: 5.320526772619975e-07\n",
      "60 th iter, loss is: 5.32052130905547e-07\n",
      "70 th iter, loss is: 5.320521277102273e-07\n",
      "80 th iter, loss is: 5.320521276900484e-07\n",
      "90 th iter, loss is: 5.320521276899181e-07\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = X.shape[0]\n",
    "for i in range(num_epochs):\n",
    "    w_grad, b_grad = get_grad(X,y,w,b)\n",
    "    w = w - lr*w_grad/N\n",
    "    b = b - lr*b_grad/N\n",
    "    los = loss(X, y, w, b)\n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', np.mean(los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.40002084],\n",
       "        [2.30000097]]), array([[4.70002848]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.4],\n",
       "        [2.3]]), 4.7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w,true_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面实现随机梯度下降法来计算模型参数  \n",
    "首先分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter1(batch_size, X, y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = np.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X[j], y[j]  # take 函数根据索引返回对应元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18685698 -0.05292217]\n",
      " [-1.05680667  0.14374236]\n",
      " [-1.01158179 -0.38416092]\n",
      " [-0.21631229 -0.41169566]\n",
      " [ 2.26181203 -1.20254096]\n",
      " [-0.571994   -1.12633831]\n",
      " [ 0.36500332 -0.76084153]\n",
      " [ 0.23405349 -0.85803565]\n",
      " [ 0.43666814  0.24758787]\n",
      " [-2.38083768 -0.88643925]] [[ 2.91490929]\n",
      " [ 3.55090278]\n",
      " [ 2.40009139]\n",
      " [ 3.45249447]\n",
      " [ 5.09940214]\n",
      " [ 1.30942582]\n",
      " [ 3.46016434]\n",
      " [ 3.05369184]\n",
      " [ 5.88066803]\n",
      " [-0.67195291]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for i,j in data_iter1(batch_size, X, y):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新初始化$w$,$b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(2,1)\n",
    "b = np.random.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: 0.027347295275899294\n",
      "10 th iter, loss is: 5.445108377788829e-07\n",
      "20 th iter, loss is: 5.397653213488734e-07\n",
      "30 th iter, loss is: 5.373639096945176e-07\n",
      "40 th iter, loss is: 5.501392030148602e-07\n",
      "50 th iter, loss is: 5.369086956030732e-07\n",
      "60 th iter, loss is: 5.385231576704392e-07\n",
      "70 th iter, loss is: 5.405155918590798e-07\n",
      "80 th iter, loss is: 5.375904719613152e-07\n",
      "90 th iter, loss is: 5.596308086531687e-07\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = X.shape[0]\n",
    "for i in range(num_epochs):\n",
    "    for X_s,y_s in data_iter1(batch_size, X, y):\n",
    "        w_grad, b_grad = get_grad(X_s,y_s,w,b)\n",
    "        w = w - lr*w_grad/batch_size\n",
    "        b = b - lr*b_grad/batch_size\n",
    "    los = loss(X, y, w, b)\n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', np.mean(los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.399912  ],\n",
       "        [2.30007441]]), array([[4.69987222]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Mxnet来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, w, b):\n",
    "    return nd.dot(X,w)+b\n",
    "def myloss(y,yhat):\n",
    "    return (y-yhat)**2/2\n",
    "def sgd(params, lr, batch_size):  # 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.randn(2,1)\n",
    "ndb = nd.random.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.1630785]\n",
       "  [0.4838046]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[0.29956347]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 100\n",
    "N = ndX.shape[0]\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有一个注意点，如果更新参数$ndw$时使用$ndw$而不是使用$ndw[:]$，需要重新申请存储梯度的内存，这是因为ndw已经指向了新的内存空间，这个变量之前申请的存储梯度内存已经失效。而$ndw[:]$的值则是写入了原来变量的内存空间，所以不需要再次申请存储梯度的内存空间（多说一句，这和python的list有所不同，$a = [1,2,3,4]$，$a[:]$是指向了新的内存空间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: [5.500447]\n",
      "10 th iter, loss is: [0.00769663]\n",
      "20 th iter, loss is: [1.6042066e-05]\n",
      "30 th iter, loss is: [5.7849286e-07]\n",
      "40 th iter, loss is: [5.322418e-07]\n",
      "50 th iter, loss is: [5.320153e-07]\n",
      "60 th iter, loss is: [5.320284e-07]\n",
      "70 th iter, loss is: [5.320284e-07]\n",
      "80 th iter, loss is: [5.320284e-07]\n",
      "90 th iter, loss is: [5.320284e-07]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    with autograd.record():\n",
    "        los = (ndy-(nd.dot(ndX, ndw) +  ndb))**2/2\n",
    "    los.backward()\n",
    "    ndw[:] = ndw-lr*ndw.grad/N\n",
    "    ndb[:] = ndb-lr*ndb.grad/N\n",
    "    '''ndw = ndw-lr*ndw.grad/N\n",
    "    ndb = ndb-lr*ndb.grad/N\n",
    "    ndw.attach_grad()\n",
    "    ndb.attach_grad()'''\n",
    "    \n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', ((ndy-(nd.dot(ndX, ndw) +  ndb))**2/2).mean().asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.4000211]\n",
       "  [2.3000007]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.700028]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet的随机梯度下降实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter2(batch_size, X, y):\n",
    "    num_examples = len(X)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield X.take(j), y.take(j)  # take 函数根据索引返回对应元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 1.1328384   0.97010976]\n",
      " [ 2.261812   -1.202541  ]\n",
      " [ 0.5233994  -2.3260677 ]\n",
      " [-0.13625841  0.04932534]\n",
      " [ 0.35480747  0.30289814]\n",
      " [ 0.18187748 -0.98623437]\n",
      " [ 1.1560868  -0.41092116]\n",
      " [-0.32945085  0.5558534 ]\n",
      " [-1.0568067   0.14374235]\n",
      " [ 0.4996388   0.7169842 ]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[[8.517731  ]\n",
      " [5.099402  ]\n",
      " [0.08239988]\n",
      " [4.6215725 ]\n",
      " [5.8935075 ]\n",
      " [2.6884203 ]\n",
      " [5.373425  ]\n",
      " [5.5161014 ]\n",
      " [3.5509028 ]\n",
      " [7.049156  ]]\n",
      "<NDArray 10x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for i,j in data_iter2(batch_size, ndX, ndy):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新初始化ndw和ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.randn(2,1)\n",
    "ndb = nd.random.randn(1,1)\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 6.100148\n",
      "epoch 2, loss 3.510528\n",
      "epoch 3, loss 2.020575\n",
      "epoch 4, loss 1.163837\n",
      "epoch 5, loss 0.669829\n",
      "epoch 6, loss 0.385972\n",
      "epoch 7, loss 0.222735\n",
      "epoch 8, loss 0.128599\n",
      "epoch 9, loss 0.074279\n",
      "epoch 10, loss 0.042887\n",
      "epoch 11, loss 0.024789\n",
      "epoch 12, loss 0.014326\n",
      "epoch 13, loss 0.008279\n",
      "epoch 14, loss 0.004782\n",
      "epoch 15, loss 0.002767\n",
      "epoch 16, loss 0.001602\n",
      "epoch 17, loss 0.000927\n",
      "epoch 18, loss 0.000535\n",
      "epoch 19, loss 0.000310\n",
      "epoch 20, loss 0.000180\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter2(batch_size, ndX, ndy):\n",
    "        with autograd.record():\n",
    "            l = myloss(net(X, ndw, ndb), y)  # l 是有关小批量 X 和 y 的损失。\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度。\n",
    "        sgd([ndw, ndb], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = myloss(net(ndX, ndw, ndb), ndy)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.3938206]\n",
       "  [2.2838824]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.689897]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw,ndb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换一种参数初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw = nd.random.normal(0,0.001,(2,1))\n",
    "ndb = nd.zeros((1,1))\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 7.770519\n",
      "epoch 2, loss 4.367507\n",
      "epoch 3, loss 2.457646\n",
      "epoch 4, loss 1.384984\n",
      "epoch 5, loss 0.781441\n",
      "epoch 6, loss 0.441956\n",
      "epoch 7, loss 0.250328\n",
      "epoch 8, loss 0.141902\n",
      "epoch 9, loss 0.080633\n",
      "epoch 10, loss 0.045870\n",
      "epoch 11, loss 0.026125\n",
      "epoch 12, loss 0.014905\n",
      "epoch 13, loss 0.008503\n",
      "epoch 14, loss 0.004857\n",
      "epoch 15, loss 0.002779\n",
      "epoch 16, loss 0.001592\n",
      "epoch 17, loss 0.000913\n",
      "epoch 18, loss 0.000524\n",
      "epoch 19, loss 0.000302\n",
      "epoch 20, loss 0.000174\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter2(batch_size, ndX, ndy):\n",
    "        with autograd.record():\n",
    "            l = myloss(net(X, ndw, ndb), y)  # l 是有关小批量 X 和 y 的损失。\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度。\n",
    "        sgd([ndw, ndb], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = myloss(net(ndX, ndw, ndb), ndy)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.3919284]\n",
       "  [2.2873595]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [[4.687408]]\n",
       " <NDArray 1x1 @cpu(0)>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw, ndb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Mxnet的高阶API Gluon来实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆一下模型的三要素\n",
    "1. 模型\n",
    "2. 损失函数\n",
    "3. 优化算法  \n",
    "  \n",
    "这几者在Gluon中都有对应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取数据，Gluon有data模块来将数据分批"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "??gdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? gdata.ArrayDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? gdata.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = gdata.ArrayDataset(ndX, ndy)\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init\n",
    "?? init.Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 7.732026\n",
      "epoch 2, loss: 4.346726\n",
      "epoch 3, loss: 2.445875\n",
      "epoch 4, loss: 1.378127\n",
      "epoch 5, loss: 0.778209\n",
      "epoch 6, loss: 0.440118\n",
      "epoch 7, loss: 0.249306\n",
      "epoch 8, loss: 0.141301\n",
      "epoch 9, loss: 0.080303\n",
      "epoch 10, loss: 0.045641\n",
      "epoch 11, loss: 0.025995\n",
      "epoch 12, loss: 0.014827\n",
      "epoch 13, loss: 0.008467\n",
      "epoch 14, loss: 0.004841\n",
      "epoch 15, loss: 0.002767\n",
      "epoch 16, loss: 0.001587\n",
      "epoch 17, loss: 0.000910\n",
      "epoch 18, loss: 0.000523\n",
      "epoch 19, loss: 0.000301\n",
      "epoch 20, loss: 0.000173\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "        l = loss(net(ndX), ndy)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.4]\n",
       "  [2.3]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [4.7]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndtrue_w, ndtrue_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = net[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1.3917496 2.2874672]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[4.687459]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

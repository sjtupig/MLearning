{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "$$\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "$$ L = \\frac{1}{2}\\sum_{i=1}^n(y_i - \\hat{y_i})^2\\tag{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据,数据有100条记录，每条记录有两个特征$x_1$和$x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([[1.4], [2.3]], dtype = np.float64)\n",
    "true_b = 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, (100,2))\n",
    "zao = np.random.randn(100,1)\n",
    "y = np.dot(X, true_w) + true_b + zao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了数据，特征$X$和对应的关注值$y$，我们要如何去得到线性模型的参数$w$和$b$呢?  \n",
    "若记$\\bar{w}=\\left[\\begin{matrix}w \\\\ b \\end{matrix}\\right]$,$\\bar{X}=\\left[\\begin{matrix}X & 1 \\end{matrix}\\right]$对于统计学，你可以推导出来最优解 $\\hat{\\bar{w}}^*=(\\bar{X}^\\intercal\\bar{X})^{-1}(\\bar{X}^\\intercal)y$。但是这个方法有很大的局限性  \n",
    "1. X必须是可逆的\n",
    "2. 当数据量和特征维度都很大是，计算耗时，求逆更耗时  \n",
    "\n",
    "那么有别的办法吗？有！从优化的角度来做。这是个凸函数，用梯度下降的方法，一定可以找到全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先计算损失函数对参数的偏导数  \n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)x_{i1}\\tag{3}$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，若定义$\\vec{w}=\\left[\\begin{matrix}w1 \\\\ w2 \\\\ b \\end{matrix}\\right]$,那么  \n",
    "$$\\frac{\\partial{L}}{\\partial{\\vec{w}}}= \\sum_{i=1}^n(\\hat{y_i}-y_i)\\left[\\begin{matrix}x_{i1}\\\\x_{i2}\\\\1 \\end{matrix}\\right]\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写成矩阵形式，对$w$的计算为, $X_{100\\times2}$ ,$y_{100\\times1}$,$\\hat{y}_{100\\times1}$\n",
    "$$\\frac{\\partial{L}}{\\partial{\\left[\\begin{matrix}w_1\\\\w_2 \\end{matrix}\\right]}} = X^\\intercal(\\hat{y}-y)\\tag{5}$$  \n",
    "对$y$的计算为,其中$\\vec{1}$元素全为1的列向量\n",
    "$$\\frac{\\partial{L}}{\\partial{y}} = {\\vec{1}}^\\intercal(\\hat{y}-y)\\tag{5}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先随机初始化$w$和$b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.86699247],\n",
       "        [ 0.01012463]]), array([0.11836671]))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.randn(2,1)\n",
    "b = np.random.randn(1)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据推导的公式计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-205.32582638],\n",
       "        [-218.83858727]]), array([[-438.51617656]]))"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_grad(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而深度学习框架都提供了自动求导机制，省去了手动推导公式的繁琐，对于神经网络等更有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd, autograd\n",
    "ndX = nd.array(X)\n",
    "ndy = nd.array(y)\n",
    "ndzao = nd.array(zao)\n",
    "ndtrue_w = nd.array(true_w)\n",
    "ndtrue_b = nd.array([true_b])\n",
    "ndw = nd.array(w)\n",
    "ndb = nd.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mxnet的变量自动求导，需要先给其分配内存记录\n",
    "ndw.attach_grad()\n",
    "ndb.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    los = (ndy-(nd.dot(ndX, ndw) +  ndb))**2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "los.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[-205.32582]\n",
       "  [-218.83862]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [-438.51617]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw.grad,ndb.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用梯度下降训练模型，这里先定义几个函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w, b):\n",
    "    return 1/2*np.mean((np.dot(X,w)+b-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求梯度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, w, b):\n",
    "    return np.dot(X.T,np.dot(X,w)+b-y), np.dot(np.ones((1,X.shape[0])),np.dot(X,w)+b-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_grad, b_grad = get_grad(X,y,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-205.32582638],\n",
       "        [-218.83858727]]), array([[-438.51617656]]))"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.86699247],\n",
       "        [ 0.01012463]]), array([0.11836671]))"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iter, loss is: 7.951563325225119\n",
      "10 th iter, loss is: 0.4581301101113125\n",
      "20 th iter, loss is: 0.44932491751667475\n",
      "30 th iter, loss is: 0.44931436383712614\n",
      "40 th iter, loss is: 0.4493143505047864\n",
      "50 th iter, loss is: 0.449314350484543\n",
      "60 th iter, loss is: 0.44931435048449586\n",
      "70 th iter, loss is: 0.44931435048449564\n",
      "80 th iter, loss is: 0.4493143504844957\n",
      "90 th iter, loss is: 0.4493143504844958\n",
      "100 th iter, loss is: 0.44931435048449564\n",
      "110 th iter, loss is: 0.44931435048449564\n",
      "120 th iter, loss is: 0.4493143504844957\n",
      "130 th iter, loss is: 0.4493143504844957\n",
      "140 th iter, loss is: 0.4493143504844957\n",
      "150 th iter, loss is: 0.4493143504844957\n",
      "160 th iter, loss is: 0.4493143504844957\n",
      "170 th iter, loss is: 0.4493143504844957\n",
      "180 th iter, loss is: 0.4493143504844957\n",
      "190 th iter, loss is: 0.4493143504844957\n",
      "200 th iter, loss is: 0.4493143504844957\n",
      "210 th iter, loss is: 0.4493143504844957\n",
      "220 th iter, loss is: 0.4493143504844957\n",
      "230 th iter, loss is: 0.4493143504844957\n",
      "240 th iter, loss is: 0.4493143504844957\n",
      "250 th iter, loss is: 0.4493143504844957\n",
      "260 th iter, loss is: 0.4493143504844957\n",
      "270 th iter, loss is: 0.4493143504844957\n",
      "280 th iter, loss is: 0.4493143504844957\n",
      "290 th iter, loss is: 0.4493143504844957\n",
      "300 th iter, loss is: 0.4493143504844957\n",
      "310 th iter, loss is: 0.4493143504844957\n",
      "320 th iter, loss is: 0.4493143504844957\n",
      "330 th iter, loss is: 0.4493143504844957\n",
      "340 th iter, loss is: 0.4493143504844957\n",
      "350 th iter, loss is: 0.4493143504844957\n",
      "360 th iter, loss is: 0.4493143504844957\n",
      "370 th iter, loss is: 0.4493143504844957\n",
      "380 th iter, loss is: 0.4493143504844957\n",
      "390 th iter, loss is: 0.4493143504844957\n",
      "400 th iter, loss is: 0.4493143504844957\n",
      "410 th iter, loss is: 0.4493143504844957\n",
      "420 th iter, loss is: 0.4493143504844957\n",
      "430 th iter, loss is: 0.4493143504844957\n",
      "440 th iter, loss is: 0.4493143504844957\n",
      "450 th iter, loss is: 0.4493143504844957\n",
      "460 th iter, loss is: 0.4493143504844957\n",
      "470 th iter, loss is: 0.4493143504844957\n",
      "480 th iter, loss is: 0.4493143504844957\n",
      "490 th iter, loss is: 0.4493143504844957\n",
      "500 th iter, loss is: 0.4493143504844957\n",
      "510 th iter, loss is: 0.4493143504844957\n",
      "520 th iter, loss is: 0.4493143504844957\n",
      "530 th iter, loss is: 0.4493143504844957\n",
      "540 th iter, loss is: 0.4493143504844957\n",
      "550 th iter, loss is: 0.4493143504844957\n",
      "560 th iter, loss is: 0.4493143504844957\n",
      "570 th iter, loss is: 0.4493143504844957\n",
      "580 th iter, loss is: 0.4493143504844957\n",
      "590 th iter, loss is: 0.4493143504844957\n",
      "600 th iter, loss is: 0.4493143504844957\n",
      "610 th iter, loss is: 0.4493143504844957\n",
      "620 th iter, loss is: 0.4493143504844957\n",
      "630 th iter, loss is: 0.4493143504844957\n",
      "640 th iter, loss is: 0.4493143504844957\n",
      "650 th iter, loss is: 0.4493143504844957\n",
      "660 th iter, loss is: 0.4493143504844957\n",
      "670 th iter, loss is: 0.4493143504844957\n",
      "680 th iter, loss is: 0.4493143504844957\n",
      "690 th iter, loss is: 0.4493143504844957\n",
      "700 th iter, loss is: 0.4493143504844957\n",
      "710 th iter, loss is: 0.4493143504844957\n",
      "720 th iter, loss is: 0.4493143504844957\n",
      "730 th iter, loss is: 0.4493143504844957\n",
      "740 th iter, loss is: 0.4493143504844957\n",
      "750 th iter, loss is: 0.4493143504844957\n",
      "760 th iter, loss is: 0.4493143504844957\n",
      "770 th iter, loss is: 0.4493143504844957\n",
      "780 th iter, loss is: 0.4493143504844957\n",
      "790 th iter, loss is: 0.4493143504844957\n",
      "800 th iter, loss is: 0.4493143504844957\n",
      "810 th iter, loss is: 0.4493143504844957\n",
      "820 th iter, loss is: 0.4493143504844957\n",
      "830 th iter, loss is: 0.4493143504844957\n",
      "840 th iter, loss is: 0.4493143504844957\n",
      "850 th iter, loss is: 0.4493143504844957\n",
      "860 th iter, loss is: 0.4493143504844957\n",
      "870 th iter, loss is: 0.4493143504844957\n",
      "880 th iter, loss is: 0.4493143504844957\n",
      "890 th iter, loss is: 0.4493143504844957\n",
      "900 th iter, loss is: 0.4493143504844957\n",
      "910 th iter, loss is: 0.4493143504844957\n",
      "920 th iter, loss is: 0.4493143504844957\n",
      "930 th iter, loss is: 0.4493143504844957\n",
      "940 th iter, loss is: 0.4493143504844957\n",
      "950 th iter, loss is: 0.4493143504844957\n",
      "960 th iter, loss is: 0.4493143504844957\n",
      "970 th iter, loss is: 0.4493143504844957\n",
      "980 th iter, loss is: 0.4493143504844957\n",
      "990 th iter, loss is: 0.4493143504844957\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 1000\n",
    "N = X.shape[0]\n",
    "for i in range(num_epochs):\n",
    "    w_grad, b_grad = get_grad(X,y,w,b)\n",
    "    w = w - lr*w_grad/N\n",
    "    b = b - lr*b_grad/N\n",
    "    los = loss(X, y, w, b)\n",
    "    if not i % 10:\n",
    "        print(i,'th iter,','loss is:', los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.30066018],\n",
       "        [2.42602976]]), array([[4.62771146]]))"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.4],\n",
       "        [2.3]]), 4.7)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w,true_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)  # take 函数根据索引返回对应元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 5.0287610e-01  5.3495818e-01]\n",
      " [-2.4273744e+00 -3.7820321e-01]\n",
      " [-5.6928647e-01  2.1097383e+00]\n",
      " [-4.3524254e-02  3.9931270e-01]\n",
      " [ 6.2985349e-01 -5.2471608e-01]\n",
      " [ 1.1761853e+00  1.6975348e+00]\n",
      " [-6.5805078e-02  1.2352763e+00]\n",
      " [-2.0735729e-03  1.8669657e+00]\n",
      " [-1.9155878e-01  1.9280884e-01]\n",
      " [-2.9792911e-01 -8.8309121e-01]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[[ 6.6100607]\n",
      " [ 1.2608424]\n",
      " [ 5.9588785]\n",
      " [ 5.9672117]\n",
      " [ 3.758598 ]\n",
      " [10.082812 ]\n",
      " [ 7.688988 ]\n",
      " [ 9.469128 ]\n",
      " [ 3.848411 ]\n",
      " [ 2.0874534]]\n",
      "<NDArray 10x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, ndX, ndy):\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  # 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "    return nd.dot(X, w) + b\n",
    "def squared_loss(y_hat, y):  # 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "def sgd(params, lr, batch_size):  # 本函数已保存在 gluonbook 包中方便以后使用。\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.252480\n",
      "epoch 2, loss 0.899208\n",
      "epoch 3, loss 0.700107\n",
      "epoch 4, loss 0.589044\n",
      "epoch 5, loss 0.527703\n",
      "epoch 6, loss 0.492404\n",
      "epoch 7, loss 0.473696\n",
      "epoch 8, loss 0.462425\n",
      "epoch 9, loss 0.456762\n",
      "epoch 10, loss 0.453340\n",
      "epoch 11, loss 0.451572\n",
      "epoch 12, loss 0.450525\n",
      "epoch 13, loss 0.450052\n",
      "epoch 14, loss 0.449882\n",
      "epoch 15, loss 0.449718\n",
      "epoch 16, loss 0.449573\n",
      "epoch 17, loss 0.449483\n",
      "epoch 18, loss 0.449394\n",
      "epoch 19, loss 0.449377\n",
      "epoch 20, loss 0.449336\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter(batch_size, ndX, ndy):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, ndw, ndb), y)  # l 是有关小批量 X 和 y 的损失。\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度。\n",
    "        sgd([ndw, ndb], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = loss(net(ndX, ndw, ndb), ndy)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[1.2964016]\n",
       "  [2.4212751]]\n",
       " <NDArray 2x1 @cpu(0)>, \n",
       " [4.629563]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndw,ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](FM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先讲一下为什么这个网络结构的结果是FM的结果  \n",
    "对于经典的FM，其公式是\n",
    "$$y = \\sum_{i=1}^{n}w_ix_i+\\sum_{i=1}^{n}\\sum_{j=i+1}^{n}<v_i,v_j>x_ix_j\\tag{1}$$  \n",
    "在deepFM中，通过FM来实现 embedding层，这里每个Field是类别特征onehot之后的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如一个类别特征是$\\left[\\begin{matrix} 1 \\\\ 3 \\\\ 0 \\end{matrix}\\right]$,它的onehot编码结果为$\\left[\\begin{matrix} 0&1&0&0 \\\\ 0&0&0&1 \\\\ 1&0&0&0 \\end{matrix}\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 1. 0. 0.]\n",
       " [0. 0. 0. 1.]\n",
       " [1. 0. 0. 0.]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "X = nd.array([[1],[3],[0]])\n",
    "X_onehot = nd.one_hot(X[:,0],depth = int(nd.max(X[:,0]).asscalar()+1))\n",
    "X_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在来看$field_i$ 和$filed_j$在这里的计算情况，假设$fild_j$有$f_i$维，$field_j$有$f_j$维，那么可以设$feild_i$的数据为$X_{n \\times field_j}$,$feild_i$的数据为$X_{n \\times field_j}$,要将$X_{n \\times field_j}$映射为$k$的隐dense向量，可以$X_{n \\times field_i}V_{field_i \\times k}$。\n",
    "$$\n",
    "V_{field_i \\times k} = \n",
    "\\left[\\begin{matrix}v_{11} & v_{12} & \\cdots & v_{1k} \\\\\n",
    "v_{21} & v_{22} & \\cdots & v_{2k}\\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots\\\\\n",
    "v_{{field_i}1} & v_{{field_i}2} & \\cdots & v_{{field_i}k}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于数据$\n",
    "\\left[\\begin{matrix}x_{11} & x_{12} & \\cdots & x_{1{field_i}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]$,它除了某一项$x_1m=1$以外，别的项$x_{1k_{(k \\ne m)}}=0$，当它和$V$点乘时有\n",
    "$$\n",
    "x_1V = \n",
    "\\left[\\begin{matrix}x_{11} & x_{12} & \\cdots & x_{1{field_i}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\\begin{matrix}v_{11} & v_{12} & \\cdots & v_{1k} \\\\\n",
    "v_{21} & v_{22} & \\cdots & v_{2k}\\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots\\\\\n",
    "v_{{field_i}1} & v_{{field_i}2} & \\cdots & v_{{field_i}k}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\\begin{matrix}v_{m1} & v_{m2} & \\cdots & v_{mk} \\\\\n",
    "\\end{matrix}\n",
    "\\right] x_{1m}\n",
    "=v_i x_{1m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，对于$field_j$有，假设$x_1n=1$\n",
    "$$\n",
    "x_1V = \n",
    "\\left[\\begin{matrix}x_{11} & x_{12} & \\cdots & x_{1{field_j}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\\begin{matrix}v_{11} & v_{12} & \\cdots & v_{1k} \\\\\n",
    "v_{21} & v_{22} & \\cdots & v_{2k}\\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots\\\\\n",
    "v_{{field_j}1} & v_{{field_j}2} & \\cdots & v_{{field_j}k}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\\begin{matrix}v_{n1} & v_{n2} & \\cdots & v_{nk} \\\\\n",
    "\\end{matrix} \n",
    "\\right] x_{1n}\n",
    "= v_j x_{1n} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是这两个$field$的dense embedding的结果为\n",
    "$$\n",
    "dense_i = \n",
    "\\left[\\begin{matrix}v_{m1} & v_{m2} & \\cdots & v_{mk} \\\\\n",
    "\\end{matrix}\n",
    "\\right] x_{1m}\n",
    "=v_i x_{1m}\n",
    "$$\n",
    "$$\n",
    "dense_j =\n",
    "\\left[\\begin{matrix}v_{n1} & v_{n2} & \\cdots & v_{nk} \\\\\n",
    "\\end{matrix}\n",
    "\\right] x_{1n}\n",
    "= v_j x_{1n}\n",
    "$$\n",
    "在FM Layer里，这些dense embeddings的结果做inner product，也就是$dense_i$和$dense_j$做向量内积。\n",
    "$$dense_i dense_j = <v_i,v_j>x_{1m}x_{1n}\n",
    "$$\n",
    "这正是$(1)$中的二阶项，而对于$i \\ne m$和$j \\ne n$，有$x_{1i}=0,x_{1j}=0$，二阶项为0。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以看到这里的dense embeddings的结果也就是FM模型中的隐向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么具体如何实现呢，从前面的分析可以知道，对单个$field$（分类变量one-hot的结果）的dense embeddings的结果就是隐向量矩阵$V$的某个行向量，也就是说，对于训练数据$X_1$,它dense embeddings的结果就是1所在的列号，在$V$中对应的行号的行向量的拼接的结果；下面提供了两种方法来实现。我们确保了他们的结果是一致的，并且在计算图中的求导也是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先生成数据，这是有一个500维的分类数据和1000维的分类数据组成，进行onehot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd, autograd\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "x1 = np.random.randint(0,500, size = (100000,1))\n",
    "x2 = np.random.randint(0,1000, size = (100000,1))\n",
    "X = nd.array(np.concatenate((x1,x2), axis = 1))\n",
    "X_onehot1 = nd.one_hot(X[:,0],depth = int(nd.max(X[:,0]).asscalar()+1))\n",
    "X_onehot2 = nd.one_hot(X[:,1],depth = int(nd.max(X[:,1]).asscalar()+1))\n",
    "X = nd.concat(X_onehot1,X_onehot2,dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成隐向量矩阵$V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = nd.random.randn(1500, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = nd.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(v1)==id(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  ...\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]]\n",
       " <NDArray 1500x5 @cpu(0)>, \n",
       " [[0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  ...\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0. 0.]]\n",
       " <NDArray 1500x5 @cpu(0)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.grad, v1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法一、使用take去取得1对应列号在$V$中行号的隐向量，再拉伸成embeddings层的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    emb = nd.take(v, nd.array(np.argwhere(X.asnumpy())[:,1].reshape(-1,2))).reshape(-1, 5*2)\n",
    "    y = nd.sum(emb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-2.7958186   2.606924    0.87531114 ...  3.561966   -3.6046371\n",
       "  4.7082334 ]\n",
       "<NDArray 100000 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[218. 218. 218. 218. 218.]\n",
       " [198. 198. 198. 198. 198.]\n",
       " [205. 205. 205. 205. 205.]\n",
       " ...\n",
       " [106. 106. 106. 106. 106.]\n",
       " [ 87.  87.  87.  87.  87.]\n",
       " [104. 104. 104. 104. 104.]]\n",
       "<NDArray 1500x5 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法二、使用直接计算的方法，要注意的是对每个$field$要单独计算，最后在讲embeddings拼接起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    t = nd.concat(nd.dot(X[:,:500],v1[:500,:]),nd.dot(X[:,500:],v1[500:,:]), dim=1)\n",
    "    y2 = nd.sum(t, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-2.7958186   2.606924    0.87531114 ...  3.561966   -3.6046371\n",
       "  4.7082334 ]\n",
       "<NDArray 100000 @cpu(0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[218. 218. 218. 218. 218.]\n",
       " [198. 198. 198. 198. 198.]\n",
       " [205. 205. 205. 205. 205.]\n",
       " ...\n",
       " [106. 106. 106. 106. 106.]\n",
       " [ 87.  87.  87.  87.  87.]\n",
       " [104. 104. 104. 104. 104.]]\n",
       "<NDArray 1500x5 @cpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到两种方法是一致的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](DNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
